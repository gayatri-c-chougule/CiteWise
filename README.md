# ğŸ“š CiteWise â€” Local AI Research Companion  

**CiteWise** is a **local-first, privacy-friendly AI assistant** that helps researchers and students work with PDF research papers.  
It extracts, chunks, embeds, and stores text in a **ChromaDB** vector database, enabling **semantic search with citation metadata** (paper + page numbers).  

---

## âœ¨ Features  

- ğŸ“„ **Upload PDFs** â€” Extract, clean, and chunk text  
- ğŸ” **Semantic Search** â€” Retrieve top-K passages using embeddings  
- ğŸ“‘ **Citation Metadata** â€” Every result includes **source file + page number(s)**  
- ğŸ’» **Local-first** â€” All processing & storage is on your machine  
- âš¡ **Configurable** â€” Model, device, and DB path via `.env`  
- ğŸ“Š **Evaluation Framework** â€” Recall/coverage scoring with CSV logs  
- ğŸ–¼ï¸ **Streamlit UI** â€” Simple and intuitive interface  

---

## ğŸ› ï¸ Tech Stack  

- **Python 3.11+**  
- **Streamlit** â€” interactive UI  
- **ChromaDB** â€” local vector database  
- **Sentence-Transformers** (`all-mpnet-base-v2`) â€” embeddings  
- **LangChain** â€” text splitting utilities  
- **PyMuPDF (fitz)** â€” PDF parsing  
- **dotenv** â€” environment variable config  

---

## ğŸš€ Quickstart  

```bash
# 1. Clone repo
git clone https://github.com/yourname/CiteWise.git
cd CiteWise

# 2. Setup venv
python -m venv venv
source venv/bin/activate   # (or venv\Scripts\activate on Windows)

# 3. Install requirements
pip install -r requirements.txt

# 4. Configure env
cp .env.example .env
# edit VECTOR_DB_PATH, EMBEDDING_MODEL if needed

# 5. Run app
streamlit run Home.py
```



## ğŸ“Š Evaluation Metrics

We evaluated CiteWise on 12 manually designed queries across two papers.  
Correctness was judged based on whether the retrieved chunks matched the expected source and page(s).

| Metric        | Value             |
|---------------|-------------------|
| **Queries**   | 12                |
| **Recall@5**  | 11 / 12 = 91.7%   |
| **Coverage**  | 9.5 / 12 â‰ˆ 79%    |

ğŸ“‚ Detailed results: [`evaluation/eval_results.csv`](evaluation/eval_results.csv)


## ğŸ“¸ Screenshots

### 1. Upload PDFs  

<p float="left">
  <img src="Screenshots/01_UploadSources.png" width="47.5%" />
  
</p>

### 2. Query & Citation Results  

<p float="left">
  <img src="Screenshots/02_FindCitations.png" width="47.5%" />
  <img src="Screenshots/03_FindCitations.png" width="47.5%" /> 
  
</p>
<p float="left">
<img src="Screenshots/04_FindCitations.png" width="47.5%" />
</p>

### 3. Evaluation Metrics  
![Evaluation Metrics](Screenshots/05_Evaluation.png)

## ğŸ—ï¸ Architecture Diagram  

The flow below illustrates how CiteWise processes research papers into a local semantic search pipeline:  

1. **PDF Upload** â†’ User uploads one or more research papers.  
2. **Chunking** â†’ Text is extracted per page, cleaned, and split into overlapping chunks.  
3. **Embedding** â†’ Each chunk is encoded into a dense vector using SentenceTransformers.  
4. **Storage** â†’ Embeddings, metadata (page, source, chunk_id), and text are stored in a local **ChromaDB** collection.  
5. **Query** â†’ User enters a natural language query.  
6. **Retrieve** â†’ Top-K similar chunks are retrieved from selected collections.  
7. **Display** â†’ Results are shown in the UI with citation metadata (source + page numbers).  

<p float="left">
  <img src="Screenshots/06_ArchitectureDiagram.png" width="47.5%" />
  
</p>


## âš ï¸ Limitations & Roadmap  

### Current Limitations  
- **No deletion of sources**: Once PDFs are embedded, they remain in ChromaDB unless manually cleared.  
- **Evaluation is semi-manual**: Current evaluation uses curated queries; automated test harness is minimal.  
- **No re-ranking or LLM integration**: Retrieval is based purely on vector similarity; no advanced reranking or reasoning step.  
- **UI simplicity**: Streamlit UI is functional but minimal, lacking features like highlighting exact match spans.  

### Roadmap  
- ğŸ“ˆ Run automated evaluation with a larger number of queries for more robust benchmarking.  
- ğŸ”„ Implement source deletion/management directly in the UI.  
- ğŸ” Explore re-ranking with lightweight LLM calls for improved citation precision.  
- ğŸ§  Add intelligent CID (character ID) replacement with optional user input/feedback loop.  


